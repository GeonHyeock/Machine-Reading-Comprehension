_target_: src.models.lamma_module.lammaaModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001


scheduler:
  _target_: src.models.components.pytorch_cosine_annealing_with_warmup.cosine_annealing_warmup.scheduler.CosineAnnealingWarmupRestarts
  _partial_: true
  first_cycle_steps: 1500
  cycle_mult: 1.0
  max_lr: 1e-4
  min_lr: 1e-5
  warmup_steps: 0
  gamma: 0.995
  start_epoch: 100
  base_lr: 1e-4

scheduler_monitor:
  monitor: "train/loss"
  interval: "step"
  frequency: 1

net:
  _target_: src.models.components.roberta.Roberta
  name: "klue/roberta-large"
  hidden_state: 1024
  is_ensemble: False
  lora_module: [self.query, self.key, self.value, output.dense]

train_param: [.]

# compile model for faster training with pytorch 2.0
compile: false
